{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:30px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong><i>Transformers</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:18px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong>Transformer</strong> modelo derivado de <strong>Seq2Seq</strong> utilizado para traducción y generación de texto.<BR><BR>\n",
    "Los modelos <strong>Seq2Seq</strong> constan de un <strong>codificador</strong> y un <strong>decodificador</strong>.<BR>\n",
    "El <strong>codificador</strong> toma la secuencia de entrada y la mapea en un espacio de multiples dimensiones.<BR><BR>\n",
    "<strong>Transformer</strong> agregar al modelo <strong>Seq2Seq</strong> la etapa de <strong>Attention</strong>.<BR>\n",
    "La etapa de <strong>Attention</strong>, analiza la entrada del modelo y determina cuál sector en la oración es más importantes, de esta forma se le indica al <strong>decodificador</strong> donde debe enfocarse.  \n",
    "</strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:20px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong>\n",
    "<u>Entrenamiento Heredado</u>:</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:17px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "La libreria <strong>tranformers</strong>, contiene multiples modelos <strong>pre-entrenados</strong> para diferentes objetivos.<BR>  <strong>Web</strong>:&nbsp;https://huggingface.co/transformers/<BR><BR>\n",
    "<strong>Modelos oficiales</strong>:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;https://huggingface.co/transformers/pretrained_models.html<BR>\n",
    "<strong>Modelos comunidad</strong>: https://huggingface.co/models.<BR></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.nn.functional'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\site-packages\\transformers\\__init__.py:2310\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __version__\n\u001b[1;32m-> 2310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\site-packages\\transformers\\file_utils.py:1656\u001b[0m, in \u001b[0;36m_BaseLazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1656\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1657\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\site-packages\\transformers\\__init__.py:2304\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 2304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\site-packages\\transformers\\pipelines\\__init__.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_tf_available, is_torch_available\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCard\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizer\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\site-packages\\transformers\\modelcard.py:31\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     CONFIG_NAME,\n\u001b[0;32m     24\u001b[0m     MODEL_CARD_NAME,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     is_remote_url,\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ALL_PRETRAINED_CONFIG_ARCHIVE_MAP\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m     35\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:71\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwav2vec2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_wav2vec2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WAV_2_VEC_2_PRETRAINED_CONFIG_ARCHIVE_MAP, Wav2Vec2Config\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxlm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_xlm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XLM_PRETRAINED_CONFIG_ARCHIVE_MAP, XLMConfig\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxlm_prophetnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_xlm_prophetnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     72\u001b[0m     XLM_PROPHETNET_PRETRAINED_CONFIG_ARCHIVE_MAP,\n\u001b[0;32m     73\u001b[0m     XLMProphetNetConfig,\n\u001b[0;32m     74\u001b[0m )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxlm_roberta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_xlm_roberta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP, XLMRobertaConfig\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxlnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_xlnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP, XLNetConfig\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\site-packages\\transformers\\models\\xlm_prophetnet\\__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_xlm_prophetnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XLMProphetNetTokenizer\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_xlm_prophetnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m         XLM_PROPHETNET_PRETRAINED_MODEL_ARCHIVE_LIST,\n\u001b[0;32m     29\u001b[0m         XLMProphetNetDecoder,\n\u001b[0;32m     30\u001b[0m         XLMProphetNetEncoder,\n\u001b[0;32m     31\u001b[0m         XLMProphetNetForCausalLM,\n\u001b[0;32m     32\u001b[0m         XLMProphetNetForConditionalGeneration,\n\u001b[0;32m     33\u001b[0m         XLMProphetNetModel,\n\u001b[0;32m     34\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\site-packages\\transformers\\models\\xlm_prophetnet\\modeling_xlm_prophetnet.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\" PyTorch XLM-ProphetNet model.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprophetnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_prophetnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     ProphetNetDecoder,\n\u001b[0;32m     20\u001b[0m     ProphetNetEncoder,\n\u001b[0;32m     21\u001b[0m     ProphetNetForCausalLM,\n\u001b[0;32m     22\u001b[0m     ProphetNetForConditionalGeneration,\n\u001b[0;32m     23\u001b[0m     ProphetNetModel,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_xlm_prophetnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XLMProphetNetConfig\n\u001b[0;32m     28\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\austral\\lib\\site-packages\\transformers\\models\\prophetnet\\modeling_prophetnet.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayerNorm\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.nn.functional'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong><i>Implementación Modelo de Preguntas y Respuestas - SQuAD2.0</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:20px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong>\n",
    "<u>Ingles</u>:</strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nlp({\n",
    "           'context':'This course is called Advanced Natural Language Processing, ' \\\n",
    "                     'the course focuses on teaching how to answer questions automatically.',\n",
    "           'question':'What topic is taught in this course ?',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{    'answer': 'Advanced Natural Language Processing, the course focuses on '\n",
      "               'teaching how to answer questions',\n",
      "     'end': 114,\n",
      "     'score': 0.4114217460155487,\n",
      "     'start': 22}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res, indent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nlp({\n",
    "           'context':'This course is called Advanced Natural Language Processing, ' \\\n",
    "                     'the course focuses on teaching how to answer questions automatically.',\n",
    "           'question':'What is the name in this course ?',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{    'answer': 'Advanced Natural Language Processing',\n",
      "     'end': 58,\n",
      "     'score': 0.9924430847167969,\n",
      "     'start': 22}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res, indent=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:20px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong>\n",
    "<u>Español</u>:</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:18px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong>Modelos comunidad</strong>: https://huggingface.co/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es<BR></p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline('question-answering', \n",
    "                model='mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',\n",
    "                tokenizer=('mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es', \n",
    "                          {'use_fast': True})\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nlp(\n",
    "    {\n",
    "        'context': 'Este curso se llama Procesamiento Avanzado de Lenguaje Natural, ' \\\n",
    "                   'el curso se enfoca en enseñar cómo responder preguntas automáticamente.',\n",
    "        'question': '¿Cual es el tema de este curso?',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'enseñar cómo responder preguntas',\n",
      " 'end': 118,\n",
      " 'score': 0.6209203004837036,\n",
      " 'start': 86}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nlp(\n",
    "    {\n",
    "        'context': 'Este curso se llama Procesamiento Avanzado de Lenguaje Natural, ' \\\n",
    "                   'el curso se enfoca en enseñar cómo responder preguntas automáticamente.',\n",
    "        'question': '¿Que se ve en este curso?',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'enseñar cómo responder preguntas automáticamente',\n",
      " 'end': 134,\n",
      " 'score': 0.31589627265930176,\n",
      " 'start': 86}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nlp(\n",
    "    {\n",
    "        'context': 'Este curso se llama Procesamiento Avanzado de Lenguaje Natural, ' \\\n",
    "                   'el curso se enfoca en enseñar cómo responder preguntas automáticamente.',\n",
    "        'question': '¿Cual se llama el curso?',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Procesamiento Avanzado de Lenguaje Natural',\n",
      " 'end': 62,\n",
      " 'score': 0.959234356880188,\n",
      " 'start': 20}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Austral",
   "language": "python",
   "name": "austral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
