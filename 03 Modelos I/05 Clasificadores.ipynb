{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunto de clasificadores robustos en NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](./img/preproc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejemplo extraido de https://scikit-learn.org/ adaptado al curso*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CountVectorizer:*** implementa la tokenización como el recuento de ocurrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Entrenamiento***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Palabras detectadas*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    " print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oraciones procesadas**\n",
    "* This is the first document.\n",
    "* This document is the second document.\n",
    "* And this is the third one.\n",
    "* Is this the first document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cada término encontrado se le asigna un **índice** entero único correspondiente a una columna en la matriz resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Document one new.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Español***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Este es el primer documento.',\n",
    "          'Este documento es el segundo documento.',\n",
    "          'Y este es el tercero.',\n",
    "          '¿Es este el primer documento?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento', 'el', 'es', 'este', 'primer', 'segundo', 'tercero']\n"
     ]
    }
   ],
   "source": [
    " print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 0 0]\n",
      " [2 1 1 1 0 1 0]\n",
      " [0 1 1 1 0 0 1]\n",
      " [1 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('este')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Algo completamente nuevo.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Primer documento nuevo.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Desventaja:*** modelo pierde informacion de contexto, oraciones en afirmativo y en interrogativo podrian ser consideradas como similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Bigrams:*** hasta cierto punto esto puede ser solucionado mediante el uso de este recurso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = vectorizer2.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oraciones procesadas**\n",
    "* This is the first document.\n",
    "* This document is the second document.\n",
    "* And this is the third one.\n",
    "* Is this the first document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Texto extenso a modelar***"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To Sherlock Holmes she is always the woman. \n",
    "I have seldom heard him mention her under any other name. \n",
    "In his eyes she eclipses and predominates the whole of her sex. \n",
    "It was not that he felt any emotion akin to love for Irene Adler. \n",
    "All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. \n",
    "He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. \n",
    "He never spoke of the softer passions, save with a gibeand a sneer. \n",
    "They were admirable things for the observer excellent for drawing the veil from men motives and actions. \n",
    "But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his. \n",
    "And yet there was but one woman to him, and that woman was the late Irene.\n",
    "Adler, of dubious and questionable memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dividing_into_sentences import read_text_file, preprocess_text, divide_into_sentences_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(filename):\n",
    "    sherlock_holmes_text = read_text_file(filename)\n",
    "    sherlock_holmes_text = preprocess_text(sherlock_holmes_text)\n",
    "    sentences = divide_into_sentences_nltk(sherlock_holmes_text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_sentence_vector(sentence, vectorizer):\n",
    "    new_sentence_vector = vectorizer.transform([sentence])\n",
    "    return new_sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(sentences):\n",
    "    vectorizer = CountVectorizer(max_df=0.6) # Se descarta lo que este por debajo de este valor\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return (vectorizer, X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram_vectorizer(sentences):\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    X = bigram_vectorizer.fit_transform(sentences)\n",
    "    return (bigram_vectorizer, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences(\"sherlock_holmes_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vectorizer, X) = create_vectorizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 113)\t1\n",
      "  (0, 98)\t1\n",
      "  (0, 46)\t1\n",
      "  (0, 97)\t1\n",
      "  (0, 53)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 123)\t1\n",
      "  (1, 38)\t1\n",
      "  (1, 94)\t1\n",
      "  (1, 40)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 63)\t1\n",
      "  (1, 41)\t1\n",
      "  (1, 115)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 78)\t1\n",
      "  (1, 69)\t1\n",
      "  (2, 97)\t1\n",
      "  (2, 41)\t1\n",
      "  (2, 47)\t1\n",
      "  (2, 45)\t1\n",
      "  (2, 28)\t1\n",
      "  (2, 24)\t1\n",
      "  (2, 87)\t1\n",
      "  :\t:\n",
      "  (9, 85)\t1\n",
      "  (9, 56)\t1\n",
      "  (9, 14)\t1\n",
      "  (9, 66)\t1\n",
      "  (9, 20)\t1\n",
      "  (9, 106)\t1\n",
      "  (9, 102)\t1\n",
      "  (9, 70)\t1\n",
      "  (10, 113)\t1\n",
      "  (10, 123)\t2\n",
      "  (10, 43)\t1\n",
      "  (10, 108)\t1\n",
      "  (10, 75)\t1\n",
      "  (10, 118)\t2\n",
      "  (10, 107)\t1\n",
      "  (10, 52)\t1\n",
      "  (10, 4)\t1\n",
      "  (10, 76)\t1\n",
      "  (10, 15)\t1\n",
      "  (10, 126)\t1\n",
      "  (10, 109)\t1\n",
      "  (10, 55)\t1\n",
      "  (10, 23)\t1\n",
      "  (10, 88)\t1\n",
      "  (10, 60)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(X.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_the_', 'abhorrent', 'actions', 'adjusted', 'adler', 'admirable', 'admirably', 'admit', 'akin', 'all', 'always', 'any', 'as', 'balanced', 'be', 'but', 'cold', 'crack', 'delicate', 'distracting', 'disturbing', 'doubt', 'drawing', 'dubious', 'eclipses', 'emotion', 'emotions', 'excellent', 'eyes', 'factor', 'false', 'felt', 'finely', 'for', 'from', 'gibe', 'grit', 'has', 'have', 'he', 'heard', 'her', 'high', 'him', 'himself', 'his', 'holmes', 'in', 'instrument', 'into', 'introduce', 'intrusions', 'irene', 'is', 'it', 'late', 'lenses', 'love', 'lover', 'machine', 'memory', 'men', 'mental', 'mention', 'might', 'mind', 'more', 'most', 'motives', 'name', 'nature', 'never', 'not', 'observer', 'observing', 'of', 'one', 'or', 'other', 'own', 'particularly', 'passions', 'perfect', 'placed', 'position', 'power', 'precise', 'predominates', 'questionable', 'reasoner', 'reasoning', 'results', 'save', 'seen', 'seldom', 'sensitive', 'sex', 'she', 'sherlock', 'sneer', 'softer', 'spoke', 'strong', 'such', 'take', 'temperament', 'than', 'that', 'the', 'there', 'they', 'things', 'throw', 'to', 'trained', 'under', 'upon', 'veil', 'was', 'were', 'which', 'whole', 'with', 'woman', 'world', 'would', 'yet']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\"\n",
    "new_sentence_vector = get_new_sentence_vector(new_sentence, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', 'and', 'that', 'woman', 'was', 'the', 'late', 'irene', 'adler', 'of', 'dubious', 'and', 'questionable', 'memory']\n"
     ]
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "print(analyze(new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 60)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 88)\t1\n",
      "  (0, 107)\t1\n",
      "  (0, 108)\t1\n",
      "  (0, 109)\t1\n",
      "  (0, 113)\t1\n",
      "  (0, 118)\t2\n",
      "  (0, 123)\t2\n",
      "  (0, 126)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 1 0 0 0 1 0 0 0 0 2 0 0 0 0 2 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_the_', 'abhorrent', 'actions', 'adjusted', 'adler', 'admirable', 'admirably', 'admit', 'akin', 'all', 'always', 'any', 'as', 'balanced', 'be', 'but', 'cold', 'crack', 'delicate', 'distracting', 'disturbing', 'doubt', 'drawing', 'dubious', 'eclipses', 'emotion', 'emotions', 'excellent', 'eyes', 'factor', 'false', 'felt', 'finely', 'for', 'from', 'gibe', 'grit', 'has', 'have', 'he', 'heard', 'her', 'high', 'him', 'himself', 'his', 'holmes', 'in', 'instrument', 'into', 'introduce', 'intrusions', 'irene', 'is', 'it', 'late', 'lenses', 'love', 'lover', 'machine', 'memory', 'men', 'mental', 'mention', 'might', 'mind', 'more', 'most', 'motives', 'name', 'nature', 'never', 'not', 'observer', 'observing', 'of', 'one', 'or', 'other', 'own', 'particularly', 'passions', 'perfect', 'placed', 'position', 'power', 'precise', 'predominates', 'questionable', 'reasoner', 'reasoning', 'results', 'save', 'seen', 'seldom', 'sensitive', 'sex', 'she', 'sherlock', 'sneer', 'softer', 'spoke', 'strong', 'such', 'take', 'temperament', 'than', 'that', 'the', 'there', 'they', 'things', 'throw', 'to', 'trained', 'under', 'upon', 'veil', 'was', 'were', 'which', 'whole', 'with', 'woman', 'world', 'would', 'yet']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bigram_vectorizer, X) = create_bigram_vectorizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 269)\t1\n",
      "  (0, 229)\t1\n",
      "  (0, 118)\t1\n",
      "  (0, 226)\t1\n",
      "  (0, 136)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 299)\t1\n",
      "  (0, 275)\t1\n",
      "  (0, 230)\t1\n",
      "  (0, 119)\t1\n",
      "  (0, 228)\t1\n",
      "  (0, 137)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 93)\t1\n",
      "  (1, 221)\t1\n",
      "  (1, 101)\t1\n",
      "  (1, 108)\t1\n",
      "  (1, 156)\t1\n",
      "  (1, 103)\t1\n",
      "  (1, 278)\t1\n",
      "  (1, 31)\t1\n",
      "  (1, 190)\t1\n",
      "  (1, 167)\t1\n",
      "  :\t:\n",
      "  (10, 307)\t1\n",
      "  (10, 261)\t1\n",
      "  (10, 141)\t1\n",
      "  (10, 60)\t1\n",
      "  (10, 210)\t1\n",
      "  (10, 151)\t1\n",
      "  (10, 30)\t1\n",
      "  (10, 308)\t1\n",
      "  (10, 262)\t1\n",
      "  (10, 285)\t1\n",
      "  (10, 45)\t1\n",
      "  (10, 187)\t1\n",
      "  (10, 300)\t1\n",
      "  (10, 271)\t1\n",
      "  (10, 109)\t1\n",
      "  (10, 251)\t1\n",
      "  (10, 301)\t1\n",
      "  (10, 288)\t1\n",
      "  (10, 253)\t1\n",
      "  (10, 142)\t1\n",
      "  (10, 8)\t1\n",
      "  (10, 180)\t1\n",
      "  (10, 61)\t1\n",
      "  (10, 27)\t1\n",
      "  (10, 211)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(X.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_the_', '_the_ woman', 'abhorrent', 'abhorrent to', 'actions', 'adjusted', 'adjusted temperament', 'adler', 'adler of', 'admirable', 'admirable things', 'admirably', 'admirably balanced', 'admit', 'admit such', 'akin', 'akin to', 'all', 'all emotions', 'all his', 'always', 'always _the_', 'and', 'and actions', 'and finely', 'and observing', 'and predominates', 'and questionable', 'and sneer', 'and that', 'and yet', 'any', 'any emotion', 'any other', 'as', 'as his', 'as lover', 'balanced', 'balanced mind', 'be', 'be more', 'but', 'but admirably', 'but as', 'but for', 'but one', 'cold', 'cold precise', 'crack', 'crack in', 'delicate', 'delicate and', 'distracting', 'distracting factor', 'disturbing', 'disturbing than', 'doubt', 'doubt upon', 'drawing', 'drawing the', 'dubious', 'dubious and', 'eclipses', 'eclipses and', 'emotion', 'emotion akin', 'emotion in', 'emotions', 'emotions and', 'excellent', 'excellent for', 'eyes', 'eyes she', 'factor', 'factor which', 'false', 'false position', 'felt', 'felt any', 'finely', 'finely adjusted', 'for', 'for drawing', 'for irene', 'for the', 'from', 'from men', 'gibe', 'gibe and', 'grit', 'grit in', 'has', 'has seen', 'have', 'have placed', 'have seldom', 'he', 'he felt', 'he never', 'he was', 'he would', 'heard', 'heard him', 'her', 'her sex', 'her under', 'high', 'high power', 'him', 'him and', 'him mention', 'himself', 'himself in', 'his', 'his cold', 'his eyes', 'his mental', 'his own', 'holmes', 'holmes she', 'in', 'in false', 'in his', 'in nature', 'in one', 'in sensitive', 'instrument', 'instrument or', 'into', 'into his', 'introduce', 'introduce distracting', 'intrusions', 'intrusions into', 'irene', 'irene adler', 'is', 'is always', 'it', 'it the', 'it was', 'late', 'late irene', 'lenses', 'lenses would', 'love', 'love for', 'lover', 'lover he', 'machine', 'machine that', 'memory', 'men', 'men motives', 'mental', 'mental results', 'mention', 'mention her', 'might', 'might throw', 'mind', 'more', 'more disturbing', 'most', 'most perfect', 'motives', 'motives and', 'name', 'nature', 'nature such', 'never', 'never spoke', 'not', 'not be', 'not that', 'observer', 'observer excellent', 'observing', 'observing machine', 'of', 'of dubious', 'of her', 'of his', 'of the', 'one', 'one of', 'one particularly', 'one woman', 'or', 'or crack', 'other', 'other name', 'own', 'own delicate', 'own high', 'particularly', 'particularly were', 'passions', 'passions save', 'perfect', 'perfect reasoning', 'placed', 'placed himself', 'position', 'power', 'power lenses', 'precise', 'precise but', 'predominates', 'predominates the', 'questionable', 'questionable memory', 'reasoner', 'reasoner to', 'reasoning', 'reasoning and', 'results', 'save', 'save with', 'seen', 'seen but', 'seldom', 'seldom heard', 'sensitive', 'sensitive instrument', 'sex', 'she', 'she eclipses', 'she is', 'sherlock', 'sherlock holmes', 'sneer', 'softer', 'softer passions', 'spoke', 'spoke of', 'strong', 'strong emotion', 'such', 'such as', 'such intrusions', 'take', 'take it', 'temperament', 'temperament was', 'than', 'than strong', 'that', 'that he', 'that one', 'that the', 'that woman', 'the', 'the late', 'the most', 'the observer', 'the softer', 'the trained', 'the veil', 'the whole', 'the world', 'there', 'there was', 'they', 'they were', 'things', 'things for', 'throw', 'throw doubt', 'to', 'to admit', 'to him', 'to his', 'to introduce', 'to love', 'to sherlock', 'trained', 'trained reasoner', 'under', 'under any', 'upon', 'upon all', 'veil', 'veil from', 'was', 'was but', 'was not', 'was take', 'was the', 'was to', 'were', 'were abhorrent', 'were admirable', 'which', 'which might', 'whole', 'whole of', 'with', 'with gibe', 'woman', 'woman to', 'woman was', 'world', 'world has', 'would', 'would have', 'would not', 'yet', 'yet there']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"I had seen little of Holmes lately.\"\n",
    "new_sentence_vector = bigram_vectorizer.transform([new_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 118)\t1\n",
      "  (0, 179)\t1\n",
      "  (0, 219)\t1\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector)\n",
    "print(new_sentence_vector.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence1 = \" And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\"\n",
    "new_sentence_vector1 = vectorizer.transform([new_sentence1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 60)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 88)\t1\n",
      "  (0, 107)\t1\n",
      "  (0, 108)\t1\n",
      "  (0, 109)\t1\n",
      "  (0, 113)\t1\n",
      "  (0, 118)\t2\n",
      "  (0, 123)\t2\n",
      "  (0, 126)\t1\n",
      "[[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 1 0 0 0 1 0 0 0 0 2 0 0 0 0 2 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector1)\n",
    "print(new_sentence_vector1.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Continuous Bag-of-Words Word2Vec:*** es una arquitectura para crear vectores de palabras que utiliza tanto palabras futuras como pasadas. La función objetivo de CBOW es:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](./img/clow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](./img/word2vec_diagrams.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Paper:*** [Efficient Estimation of Word Representations in Vector Space](https://paperswithcode.com/method/cbow-word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Codigo adaptado para la clase: ***fasttext*** quick start guide*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras.backend as K\n",
    "#import keras.backend.tensorflow_backend as K\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Lambda, Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgrams(sequence, vocabulary_size,\n",
    "              window_size=window_size, negative_samples=1., shuffle=True,\n",
    "              categorical=False, sampling_table=None, seed=None):\n",
    "    couples = []\n",
    "    labels = []\n",
    "    for i, wi in enumerate(sequence):\n",
    "        if not wi:\n",
    "            continue\n",
    "        if sampling_table is not None:\n",
    "            if sampling_table[wi] < random.random():\n",
    "                continue\n",
    "\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "        for j in range(window_start, window_end):\n",
    "            if j != i:\n",
    "                wj = sequence[j]\n",
    "                if not wj:\n",
    "                    continue\n",
    "                couples.append([wi, wj])\n",
    "                if categorical:\n",
    "                    labels.append([0, 1])\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "    if negative_samples > 0:\n",
    "        num_negative_samples = int(len(labels) * negative_samples)\n",
    "        words = [c[0] for c in couples]\n",
    "        random.shuffle(words)\n",
    "\n",
    "        couples += [[words[i % len(words)],\n",
    "                    random.randint(1, vocabulary_size - 1)]\n",
    "                    for i in range(num_negative_samples)]\n",
    "        if categorical:\n",
    "            labels += [[1, 0]] * num_negative_samples\n",
    "        else:\n",
    "            labels += [0] * num_negative_samples\n",
    "\n",
    "    if shuffle:\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 10e6)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(couples)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(labels)\n",
    "        \n",
    "    return couples, labels\n",
    "\n",
    "def generate_data_for_cbow(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    corpus = tokenizer.texts_to_sequences(corpus)\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/alice.txt'\n",
    "corpus = open(path, encoding=\"utf-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in corpus if sentence.count(' ') >= 2]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "V=len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "# inputs\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "w = Embedding(V, embedding_dim)(w_inputs)\n",
    "\n",
    "# context\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "c = Embedding(V, embedding_dim)(c_inputs)\n",
    "o = Dot(axes=2)([w, c])\n",
    "o = Reshape((1,), input_shape=(1, 1))(o)\n",
    "o = Activation('sigmoid')(o)\n",
    "\n",
    "sg_model = Model(inputs=[w_inputs, c_inputs], outputs=o)\n",
    "sg_model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=embedding_dim, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Entrenamiento***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1110.177969634533\n",
      "1 758.7284075319767\n",
      "2 702.7573907524347\n",
      "3 674.9359678328037\n",
      "4 651.4569097012281\n"
     ]
    }
   ],
   "source": [
    "for ite in range(5):\n",
    "    loss = 0.\n",
    "    for i, doc in enumerate(tokenizer.texts_to_sequences(corpus)):\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=5, negative_samples=5.)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        if x:\n",
    "            loss += sg_model.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 250922.2698507309\n",
      "1 250793.67269039154\n",
      "2 250664.94546985626\n",
      "3 250535.95819091797\n",
      "4 250406.54448890686\n"
     ]
    }
   ],
   "source": [
    "for ite in range(5):\n",
    "    loss  =  0.\n",
    "    for  x, y in generate_data_for_cbow(corpus, window_size, V):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Guardar los vectores generados***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/sg_vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(V-1, embedding_dim))\n",
    "    vectors = sg_model.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cbow_vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(V-1, embedding_dim))\n",
    "    vectors = cbow.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Carga de vectores***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_model = gensim.models.KeyedVectors.load_word2vec_format( open('./data/sg_vectors.txt', 'r'), binary=False)\n",
    "cbow_model = gensim.models.KeyedVectors.load_word2vec_format(open('./data/cbow_vectors.txt', 'r'), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hearts', 0.7938040494918823),\n",
       " ('became', 0.7249054312705994),\n",
       " ('cook', 0.7186954617500305),\n",
       " ('verse', 0.689565122127533),\n",
       " ('impatiently', 0.6888817548751831),\n",
       " ('king', 0.6803977489471436),\n",
       " ('lobsters', 0.67889004945755),\n",
       " ('wildly', 0.6765428185462952),\n",
       " ('footman', 0.6761890649795532),\n",
       " ('duchess’s', 0.6755886673927307)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.most_similar(positive=['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 0.3321659564971924),\n",
       " ('impatiently', 0.32813021540641785),\n",
       " ('sir—”', 0.3218364715576172),\n",
       " ('wish', 0.31456565856933594),\n",
       " ('rule', 0.311204195022583),\n",
       " ('he’ll', 0.3093535304069519),\n",
       " ('energetic', 0.30847007036209106),\n",
       " ('gravely', 0.3012162744998932),\n",
       " ('elbow', 0.28901851177215576),\n",
       " ('eats', 0.2877623438835144)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.most_similar(positive=['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thought', 0.6757129430770874),\n",
       " ('poor', 0.6078629493713379),\n",
       " ('“it', 0.5983031988143921),\n",
       " ('“i’m', 0.5915191173553467),\n",
       " ('rather', 0.5902792811393738),\n",
       " ('curious', 0.5878759622573853),\n",
       " ('doubtfully', 0.5791394114494324),\n",
       " ('girl', 0.5791118144989014),\n",
       " ('“that’s', 0.5784812569618225),\n",
       " ('“i’d', 0.5708906650543213)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.most_similar(positive=['alice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('produced', 0.36737269163131714),\n",
       " ('feeling', 0.3586391806602478),\n",
       " ('stay', 0.3092033863067627),\n",
       " ('“who', 0.29888805747032166),\n",
       " ('absurd', 0.29571735858917236),\n",
       " ('book', 0.2882564961910248),\n",
       " ('sad', 0.27913543581962585),\n",
       " ('bread', 0.2776448428630829),\n",
       " ('future', 0.27288612723350525),\n",
       " ('daisy', 0.27287036180496216)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.most_similar(positive=['alice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hearts', 0.6624538898468018),\n",
       " ('queen', 0.6542152166366577),\n",
       " ('queen’s', 0.653067946434021),\n",
       " ('king', 0.6318836212158203),\n",
       " ('under', 0.5902504324913025),\n",
       " ('laws', 0.5705169439315796),\n",
       " ('professor', 0.5625193119049072),\n",
       " ('march', 0.5595977902412415),\n",
       " ('singers', 0.5542469024658203),\n",
       " ('became', 0.5508366823196411)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.most_similar(positive=['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('preserve', 0.34494319558143616),\n",
       " ('fact', 0.3368389904499054),\n",
       " ('brought', 0.33671051263809204),\n",
       " ('carrier', 0.3263804316520691),\n",
       " ('by', 0.31738507747650146),\n",
       " ('answered', 0.29890382289886475),\n",
       " ('royalty', 0.2966153621673584),\n",
       " ('soldiers', 0.28517836332321167),\n",
       " ('dreamy', 0.2848338484764099),\n",
       " ('whistle', 0.2828007936477661)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.most_similar(positive=['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term-Frequency Inverse Document-Frequency - TF IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un corpus de texto extenso, algunas palabras estarán muy presentes (por ejemplo, \"the\", \"a\", \"is\" en inglés), por lo que ***contienen muy poca información significativa sobre el contenido real del documento***. Si se pasa directamente a un clasificador, esos términos muy frecuentes ensombrecerían las frecuencias de términos más raros pero más interesantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from removing_stopwords import read_in_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stopwords_file_path = \"./data/stopwords.csv\"\n",
    "sentences = get_sentences(\"sherlock_holmes_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_vectorizer(sentences):\n",
    "    #Create TF-IDF object\n",
    "    tfidf_char_vectorizer = TfidfVectorizer(analyzer='char_wb', max_df=0.90, max_features=200000,\n",
    "                                        min_df=0.05, use_idf=True, ngram_range=(1,3))\n",
    "    tfidf_char_vectorizer = tfidf_char_vectorizer.fit(sentences)\n",
    "    tfidf_matrix = tfidf_char_vectorizer.transform(sentences)\n",
    "    print(tfidf_matrix)\n",
    "    dense_matrix = tfidf_matrix.todense()\n",
    "    print(dense_matrix)\n",
    "    print(tfidf_char_vectorizer.get_feature_names())\n",
    "    analyze = tfidf_char_vectorizer.build_analyzer()\n",
    "    print(analyze(\"To Sherlock Holmes she is always _the_ woman.\"))\n",
    "    return (tfidf_char_vectorizer, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(sentences):\n",
    "    #Create TF-IDF object\n",
    "    stopword_list = read_in_csv(stopwords_file_path)\n",
    "    stemmed_stopwords = [tokenize_and_stem(stopword)[0] for stopword in stopword_list]\n",
    "    stopword_list = stopword_list + stemmed_stopwords\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.90, max_features=200000,\n",
    "                                        min_df=0.05, stop_words=stopword_list,\n",
    "                                        use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "    tfidf_vectorizer = tfidf_vectorizer.fit(sentences)\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(sentences)\n",
    "    print(tfidf_matrix)\n",
    "    dense_matrix = tfidf_matrix.todense()\n",
    "    print(dense_matrix)\n",
    "    print(tfidf_vectorizer.get_feature_names())\n",
    "    return (tfidf_vectorizer, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/stopwords.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (vectorizer, matrix) \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [79]\u001b[0m, in \u001b[0;36mcreate_vectorizer\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_vectorizer\u001b[39m(sentences):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#Create TF-IDF object\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     stopword_list \u001b[38;5;241m=\u001b[39m \u001b[43mread_in_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopwords_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     stemmed_stopwords \u001b[38;5;241m=\u001b[39m [tokenize_and_stem(stopword)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m stopword \u001b[38;5;129;01min\u001b[39;00m stopword_list]\n\u001b[0;32m      5\u001b[0m     stopword_list \u001b[38;5;241m=\u001b[39m stopword_list \u001b[38;5;241m+\u001b[39m stemmed_stopwords\n",
      "File \u001b[1;32mH:\\univ\\Austral\\Text Mining Rs II\\removing_stopwords.py:6\u001b[0m, in \u001b[0;36mread_in_csv\u001b[1;34m(csv_file)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_in_csv\u001b[39m(csv_file):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m      7\u001b[0m         reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(fp, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m         data_read \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/stopwords.csv'"
     ]
    }
   ],
   "source": [
    "(vectorizer, matrix) = create_vectorizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "print(analyze(\"To Sherlock Holmes she is always _the_ woman.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = newsgroups_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = newsgroups_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Categorias de las 20 fuentes de datos:\")\n",
    "print (newsgroups_train.target_names)\n",
    "print (\"___________________________\")\n",
    "print (\"Ejemplo de un email:\")\n",
    "print (x_train[0])\n",
    "print (\"___________________________\")\n",
    "print (\"Ejemplos de Target:\")\n",
    "print (y_train[0])\n",
    "print (newsgroups_train.target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    try:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    except:\n",
    "        tokens = tokens\n",
    "        \n",
    "    tagged_corpus = pos_tag(tokens)    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    \n",
    "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
    "\n",
    "    return pre_proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_train:\n",
    "    x_train_preprocessed.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_preprocessed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_test:\n",
    "    x_test_preprocessed.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),  stop_words='english', \n",
    "                             max_features= 10000,strip_accents='unicode',  norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adadelta,Adam,RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyper parameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337) \n",
    "nb_classes = 20\n",
    "batch_size = 64\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1000,input_shape= (10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predclass = model.predict_classes(x_train_2,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predclass = model.predict_classes(x_test_2,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Train accuracy: {}\". format(round(accuracy_score(y_train,y_train_predclass),3)))\n",
    "print (\"Test accuracy: {}\". format(round(accuracy_score(y_test,y_test_predclass),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Test Classification Report\\n\")\n",
    "print (classification_report(y_test,y_test_predclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mecanismo que como resultado del entrenamiento de una ***red neuronal***, predice una palabra a partir de todas las demás palabras de la oración. Los vectores resultantes son similares para palabras que ocurren en contextos similares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_model_path = \"./models/40/model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = KeyedVectors.load_word2vec_format(w2vec_model_path, binary=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(word_vectors):\n",
    "    matrix = np.array(word_vectors)\n",
    "    centroid = np.mean(matrix[:,:], axis=0)\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(sentence, model):\n",
    "    word_vectors = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            word_vector = model.get_vector(word.lower())\n",
    "            word_vectors.append(word_vector)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(w2vec_model_path)\n",
    "print(model['holmes'])\n",
    "print(model.most_similar(['holmes'], topn=15))\n",
    "\n",
    "sentence = \"It was not that he felt any emotion akin to love for Irene Adler.\"\n",
    "word_vectors = get_word_vectors(sentence, model)\n",
    "sentence_vector = get_sentence_vector(word_vectors)\n",
    "words = ['banana', 'apple', 'computer', 'strawberry']\n",
    "print(model.doesnt_match(words))\n",
    "\n",
    "word = \"cup\"\n",
    "words = ['glass', 'computer', 'pencil', 'watch']\n",
    "print(model.most_similar_to_given(word, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo ***word2vec*** utiliza un modelo de red neuronal para aprender asociaciones de palabras de un gran corpus de texto. Una vez entrenado, dicho modelo puede detectar palabras sinónimas o sugerir palabras adicionales para una oración parcial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](./img/word2vec_translation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuente: Python Deep Learning Projects, curaduria para el presente curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\", \" \", raw)\n",
    "    words = clean.split()\n",
    "    return list(map(lambda x: x.lower(), words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preproceso a realizar con el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](./img/learning-word-vectors-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Texto extraido de Principles of Geology by Sir Charles Lyell, Project Gutenberg***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'http://www.gutenberg.org/files/33224/33224-0.txt'\n",
    "corpus_raw = requests.get(filepath).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence where each word is tokenized\n",
    "sentences = (sentence_to_wordlist(raw) for raw in raw_sentences if raw)\n",
    "sentences = list(sentences)\n",
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(f'The book corpus contains {token_count} tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definicion del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones\n",
    "num_features = 300\n",
    "\n",
    "# umbral minimo para considerar una palabra.\n",
    "min_word_count = 3\n",
    "\n",
    "# Definicon de tareas en paralelo\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Ventana \n",
    "context_size = 7\n",
    "\n",
    "# Bajada a disco\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Semilla.\n",
    "seed = 1\n",
    "\n",
    "model2vec = w2v.Word2Vec(sg=1, seed=seed, workers=num_workers, min_count=min_word_count,\n",
    "                         window=context_size, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2vec.build_vocab(list(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Nota:*** Aumentar el número de dimensiones conduce a una mejor generalización, pero también agrega más complejidad computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Nota:*** Parametro ***context_size***. Establece el límite superior para la distancia entre la predicción de palabras actual y objetivo dentro de una oración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2vec.train(sentences, total_examples=model2vec.corpus_count, epochs=10)\n",
    "\n",
    "if not os.path.exists(os.path.join('trained', 'sample')):\n",
    "    os.makedirs(os.path.join('trained', 'sample'))\n",
    "\n",
    "model2vec.save(os.path.join('trained', 'sample', 'sample.w2v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Similar a: \"earth\":')\n",
    "for sWord in model2vec.wv.most_similar(\"earth\"):\n",
    "    print(sWord)\n",
    "    \n",
    "print('\\nSimilar a: \"human\":')\n",
    "for sWord in model2vec.wv.most_similar(\"human\"):\n",
    "    print(sWord)\n",
    "    \n",
    "print('\\nContribucion Positiva y Negativa:')\n",
    "for sWord in model2vec.wv.most_similar_cosmul(positive=['earth', 'moon'], negative=['orbit']):\n",
    "    print(sWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tokenization import tokenize_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from bag_of_words import get_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word2vec_model_path = \"models/word2vec.model\"\n",
    "books_dir = \"books/\"\n",
    "evaluation_file = \"questions-words.txt\"\n",
    "pretrained_model_path = \"models/40/model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_word2vec(words, word2vec_model_path):\n",
    "    #model = gensim.models.Word2Vec(\n",
    "    #    words,\n",
    "    #    size=50,\n",
    "    #    window=7,\n",
    "    #    min_count=1,\n",
    "    #    workers=10)\n",
    "    model = gensim.models.Word2Vec(words, window=5, min_count=5)\n",
    "    model.train(words, total_examples=len(words), epochs=200)\n",
    "    pickle.dump(model, open(word2vec_model_path, 'wb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_all_book_sentences(directory):\n",
    "    text_files = [join(directory, f) for f in listdir(directory) if isfile(join(directory, f)) and \".rtf\" in f]\n",
    "    all_sentences = []\n",
    "    for text_file in text_files:\n",
    "        sentences = get_sentences(text_file)\n",
    "        all_sentences = all_sentences + sentences\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_model(w1):\n",
    "    model = pickle.load(open(word2vec_model_path, 'rb'))\n",
    "    #words = list(model.wv.vocab)\n",
    "    words = list(model.wv.index_to_key)\n",
    "    #print(words)\n",
    "    words = model.wv.most_similar(w1, topn=10)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, filename):\n",
    "    return model.wv.accuracy(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['books/Adventures_of_Huckleberry_Finn_by_Mark_Twain.rtf', 'books/Alices_Adventures_in_Wonderland_by_Lewis_Carroll.rtf', 'books/A_Dolls_House_by_Henrik_Ibsen.rtf', 'books/A_Tale_of_Two_Cities_by_Charles_Dickens.rtf', 'books/Dracula_by_Bram_Stoker.rtf', 'books/Emma_by_Jane_Austen.rtf', 'books/Frankenstein_by_Mary_Shelley.rtf', 'books/Great_Expectations_by_Charles_Dickens.rtf', 'books/Grimms_Fairy_Tales_by_The_Brothers_Grimm.rtf', 'books/Metamorphosis_by_Franz_Kafka.rtf', 'books/Pride_and_Prejudice_by_Jane_Austen.rtf', 'books/The_Adventures_of_Sherlock_Holmes_by_Arthur_Conan_Doyle.rtf', 'books/The_Adventures_of_Tom_Sawyer_by_Mark_Twain.rtf', 'books/The_Count_of_Monte_Cristo_by_Alexandre_Dumas.rtf', 'books/The_Importance_of_Being_Earnest_by_Oscar_Wilde.rtf', 'books/The_Picture_of_Dorian_Gray_by_Oscar_Wilde.rtf', 'books/The_Prince_by_Nicolo_Machiavelli.rtf', 'books/The_Romance_of_Lust_by_Anonymous.rtf', 'books/The_Yellow_Wallpaper_by_Charlotte_Perkins_Gilman.rtf', 'books/Ulysses_by_James_Joyce.rtf']\n"
     ]
    }
   ],
   "source": [
    "sentences = get_all_book_sentences(books_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = [tokenize_nltk(s.lower()) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = train_word2vec(sentences,word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('banks', 0.6442744731903076), ('stream', 0.6146652102470398), ('woods', 0.6124129891395569), ('hill', 0.6082079410552979), ('illinois', 0.6081189513206482), ('shore', 0.6002070307731628), ('mountains', 0.5959462523460388), ('avenue', 0.5911605954170227), ('corso', 0.5810585021972656), ('raft', 0.5785054564476013)]\n"
     ]
    }
   ],
   "source": [
    "oneWord = \"river\"\n",
    "test_model(oneWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = pickle.load(open(word2vec_model_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Word2Vec Español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Entrenamiento de wor2vect en españo, ejemplo adaptado para la clase](https://github.com/dccuchile/spanish-word-embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Alternativamente s epuede acceder a [Word vectors for 157 languages](https://fasttext.cc/docs/en/crawl-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wordvectors_file_vec = './data/fasttext-sbwc.3.6.e20.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cantidad = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Buscar analogias o palabras que tienen un contexto similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Reference: KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reina', 0.9141532778739929),\n",
       " ('infanta', 0.8582409620285034),\n",
       " ('berenguela', 0.8470728993415833),\n",
       " ('princesa', 0.8445042371749878),\n",
       " ('consorte', 0.835599422454834),\n",
       " ('emperatriz', 0.8247664570808411),\n",
       " ('regente', 0.8239888548851013),\n",
       " ('infantas', 0.8104740381240845),\n",
       " ('hermanastra', 0.8072930574417114),\n",
       " ('regencia', 0.8037239909172058)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['rey','mujer'],negative=['hombre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actriz', 0.9687139391899109),\n",
       " ('compositora', 0.855713427066803),\n",
       " ('cantante', 0.8482002019882202),\n",
       " ('actrices', 0.845941424369812),\n",
       " ('dramaturga', 0.8354867696762085),\n",
       " ('presentadora', 0.8346402645111084),\n",
       " ('bailarina', 0.8301039934158325),\n",
       " ('coprotagonista', 0.8284398317337036),\n",
       " ('guionista', 0.828334629535675),\n",
       " ('cantautora', 0.8273791670799255)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['actor','mujer'],negative=['hombre'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hija', 0.9641352295875549),\n",
       " ('esposa', 0.911634087562561),\n",
       " ('madre', 0.9057635068893433),\n",
       " ('nieta', 0.8976945877075195),\n",
       " ('hermanastra', 0.8958925604820251)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['hijo','mujer'],negative=['hombre'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nuera', 0.8991931080818176),\n",
       " ('cuñada', 0.8967029452323914),\n",
       " ('esposa', 0.8791162967681885),\n",
       " ('hija', 0.8787108659744263),\n",
       " ('suegra', 0.8752366304397583),\n",
       " ('sobrina', 0.8678680658340454),\n",
       " ('hermanastra', 0.8615662455558777),\n",
       " ('viuda', 0.8587483167648315),\n",
       " ('yernos', 0.8577941656112671),\n",
       " ('nieta', 0.8574916124343872)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['yerno','mujer'],negative=['hombre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('juega', 0.927038848400116),\n",
       " ('jugará', 0.9030497670173645),\n",
       " ('juegue', 0.8957996368408203),\n",
       " ('jugando', 0.8832089304924011),\n",
       " ('juegan', 0.868077278137207),\n",
       " ('jugado', 0.8658615946769714),\n",
       " ('jugó', 0.8645128607749939),\n",
       " ('juegas', 0.8533657789230347),\n",
       " ('jugaría', 0.8508267402648926),\n",
       " ('jugara', 0.8470849394798279)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['jugar','canta'],negative=['cantar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jugaría', 1.002570629119873),\n",
       " ('jugarían', 0.9512909650802612),\n",
       " ('jugara', 0.9422452449798584),\n",
       " ('disputaría', 0.918655276298523),\n",
       " ('jugará', 0.908361554145813),\n",
       " ('jugaran', 0.8989545106887817),\n",
       " ('jugase', 0.8874877095222473),\n",
       " ('disputarían', 0.8822468519210815),\n",
       " ('jugó', 0.8740343451499939),\n",
       " ('ficharía', 0.8733251094818115)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['jugar','cantaría'],negative=['cantar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yendo', 0.907002329826355),\n",
       " ('ido', 0.8450857996940613),\n",
       " ('saliendo', 0.832144021987915),\n",
       " ('caminando', 0.8135581612586975),\n",
       " ('yéndose', 0.8133329153060913),\n",
       " ('acercando', 0.8035196661949158),\n",
       " ('iremos', 0.8023999333381653),\n",
       " ('marchando', 0.8001841902732849),\n",
       " ('parando', 0.7995682954788208),\n",
       " ('irá', 0.7987060546875)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['ir','jugando'],negative=['jugar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caracas', 0.9048638343811035),\n",
       " ('barinas', 0.871845543384552),\n",
       " ('brión', 0.8565776944160461),\n",
       " ('cojedes', 0.851475715637207),\n",
       " ('cumaná', 0.8507834076881409),\n",
       " ('guanare', 0.8507249355316162),\n",
       " ('maturín', 0.8474243879318237),\n",
       " ('mariño', 0.8468520641326904),\n",
       " ('barquisimeto', 0.8451403379440308),\n",
       " ('falcón', 0.8430415987968445)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['santiago','venezuela'],negative=['chile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cuba', 0.9638005495071411),\n",
       " ('venezuela', 0.8891815543174744),\n",
       " ('colombia', 0.876230001449585),\n",
       " ('cubana', 0.8471046686172485),\n",
       " ('nicaragua', 0.8443881273269653),\n",
       " ('cubanos', 0.8370179533958435),\n",
       " ('ecuador', 0.8361554145812988),\n",
       " ('brasil', 0.8355840444564819),\n",
       " ('cubano', 0.8315702080726624),\n",
       " ('panamá', 0.8302189111709595)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['habana','chile'],negative=['santiago'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chile'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_to_given('santiago', ['cuba','chile', 'brasil'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7982443"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.n_similarity('santiago', 'chile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9034706"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.n_similarity('chile', 'brasil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### palabra dentro de que está más lejana del resto de las palabras de la lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chile'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.doesnt_match(['blanco','azul','rojo','chile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'almuerzo'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.doesnt_match(['sol','luna','almuerzo','jupiter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bidirectional Encoder Representations from Transformers - Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***BERT*** utiliza Transformer, un mecanismo que aprende las relaciones contextuales entre palabras en un texto. En su forma básica, Transformer incluye dos mecanismos separados: un codificador que lee la entrada de texto y un decodificador que produce una predicción para la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from dividing_into_sentences import read_text_file, divide_into_sentences_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text = read_text_file(\"sherlock_holmes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = divide_into_sentences_nltk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f446596ee44e188788f7b88f21fe2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Admin/.cache\\torch\\sentence_transformers\\sbert.net_models_bert-base-nli-mean-tokens\\0_BERT were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode([\"the beautiful lake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "[[-7.61979818e-02 -5.74670196e-01  1.08264279e+00  7.36554265e-01\n",
      "   5.51345468e-01 -9.39117730e-01 -2.80430317e-01 -5.41625619e-01\n",
      "   7.50949025e-01 -4.40971524e-01  5.31526685e-01 -5.41883469e-01\n",
      "   1.92792729e-01  3.44117790e-01  1.50266433e+00 -6.26989603e-01\n",
      "  -2.42828488e-01 -3.66734445e-01  5.57459652e-01 -2.21802518e-01\n",
      "  -9.69591320e-01 -4.38950866e-01 -7.93552220e-01 -5.84923148e-01\n",
      "  -1.55690819e-01  2.12004021e-01  4.02013242e-01 -2.63063669e-01\n",
      "   6.21910155e-01  5.97237468e-01  9.78126079e-02  7.20052540e-01\n",
      "  -4.66322601e-01  3.86450440e-01 -8.24903786e-01  1.09985721e+00\n",
      "  -3.59134972e-01 -4.31918532e-01  2.56567001e-02  5.73159456e-01\n",
      "   2.40237385e-01 -7.67571270e-01  9.38899636e-01 -3.60024512e-01\n",
      "  -8.77114773e-01 -2.47680500e-01 -8.65839303e-01  1.04203498e+00\n",
      "   3.65989506e-01 -6.47719055e-02 -7.04246700e-01  5.91062289e-03\n",
      "  -8.04807484e-01  2.21369982e-01 -1.79775178e-01  8.04759383e-01\n",
      "  -4.44357067e-01 -4.46379125e-01  7.55990148e-02 -2.17623919e-01\n",
      "   6.87522233e-01 -4.70606327e-01  7.68602073e-01  3.06245744e-01\n",
      "  -9.10274327e-01  6.28714561e-01  8.11625004e-01 -3.83964702e-02\n",
      "   2.41826892e-01 -3.13487530e-01  9.08200622e-01  9.62718278e-02\n",
      "  -4.04239476e-01  3.88271809e-01 -4.22080338e-01 -4.33439165e-01\n",
      "   7.79737413e-01 -1.52796358e-01  9.48364615e-01  9.40597892e-01\n",
      "   7.34893441e-01  6.64678156e-01  3.90085220e-01  1.34034216e+00\n",
      "   1.08233236e-01 -3.95383537e-01  2.27669045e-01  2.79989243e-01\n",
      "  -1.70993876e+00  4.19878662e-01  2.10107952e-01 -5.28874516e-01\n",
      "   7.26883471e-01 -1.09560490e+00 -1.37760248e-02  9.04117882e-01\n",
      "   3.81057486e-02 -2.46198654e-01  5.54345667e-01 -3.61580938e-01\n",
      "   6.56911552e-01 -1.13580954e+00 -3.63160372e-01  1.89225242e-01\n",
      "  -9.07701612e-01  4.76393789e-01  5.17776489e-01 -1.19439948e+00\n",
      "   9.81190085e-01 -4.90278974e-02 -1.50155425e-01  5.42353034e-01\n",
      "   7.50451505e-01  4.16537344e-01 -8.21167976e-02  1.74671099e-01\n",
      "  -1.52958214e+00  7.14822888e-01  2.90298283e-01  7.76944399e-01\n",
      "  -3.69304419e-01 -5.91735356e-02  4.75242063e-02 -5.92363067e-02\n",
      "   5.25073409e-01 -7.08114445e-01 -2.52816617e-01  1.29740536e+00\n",
      "  -1.28921819e+00  1.40943319e-01  2.23808214e-01  3.13144803e-01\n",
      "   5.11783957e-01 -2.58186795e-02 -2.22353395e-02  1.43362552e-01\n",
      "   3.71723324e-01 -8.38745475e-01 -1.68651581e+00  1.41473994e-01\n",
      "  -1.02585101e+00  2.03563094e-01  2.62083560e-01  7.15595335e-02\n",
      "   8.99083436e-01 -5.28086424e-01  4.57701027e-01 -1.53959095e-01\n",
      "  -1.18477866e-01 -8.72816592e-02  3.76759283e-02  7.07110405e-01\n",
      "  -7.49196231e-01  6.55256212e-02  5.15916169e-01  8.10096085e-01\n",
      "  -5.19098520e-01  2.18548343e-01  3.93635422e-01  6.09233201e-01\n",
      "   3.38943481e-01  9.46189523e-01 -1.22255194e+00 -1.30253091e-01\n",
      "   6.54754102e-01 -8.41753483e-01  8.06664452e-02  3.00382644e-01\n",
      "  -8.12576473e-01  5.76909959e-01  3.84842187e-01 -4.66208398e-01\n",
      "   2.26863436e-02 -1.82356864e-01  3.94846976e-01 -4.98396695e-01\n",
      "  -3.28548968e-01 -9.03554559e-01  8.20927992e-02 -1.04095960e+00\n",
      "  -2.30055854e-01 -3.92115653e-01  4.96068865e-01 -2.35530406e-01\n",
      "  -4.00794268e-01 -6.51569366e-01 -2.91195095e-01  1.30913883e-01\n",
      "   4.64919567e-01 -3.41870308e-01  2.17010885e-01 -1.27972937e+00\n",
      "  -4.89798337e-02 -2.56777346e-01  5.36798716e-01  1.87181935e-01\n",
      "  -1.41931966e-01  1.18326187e+00 -2.36010715e-01 -4.17852074e-01\n",
      "   7.82344222e-01  2.52221137e-01 -5.96156299e-01 -1.00841381e-01\n",
      "  -8.53247821e-01 -4.25653644e-02  1.98200136e-01 -4.74220663e-01\n",
      "  -7.92613149e-01 -5.80055527e-02 -5.30738771e-01 -4.08136770e-02\n",
      "   1.06413805e+00 -1.95783705e-01 -1.18166244e+00 -3.84621561e-01\n",
      "   1.83776230e-01  1.07725384e-02 -5.73451638e-01 -6.28655627e-02\n",
      "   3.68158996e-01  2.95110755e-02 -3.12129200e-01  7.59689271e-01\n",
      "   1.71069980e-01 -2.51229405e-01  1.21716094e+00 -1.22804523e-01\n",
      "  -1.25155675e+00  2.96520233e-01 -1.59363210e-01 -3.07772458e-01\n",
      "   7.38719940e-01 -3.21678609e-01  4.56360638e-01  6.06843114e-01\n",
      "  -1.03566349e-01  5.64707518e-01  1.14319158e+00  1.58939615e-01\n",
      "   4.00118172e-01 -1.40942201e-01  1.12744547e-01 -5.38727045e-01\n",
      "  -1.14471221e+00  2.97611982e-01  9.01935697e-01  1.17314294e-01\n",
      "   1.23890564e-01  1.81390613e-01  2.68770635e-01 -1.31406009e-01\n",
      "  -7.04886690e-02 -1.44888532e+00  6.95817471e-01  9.22024071e-01\n",
      "  -3.09454530e-01 -3.62523884e-01 -6.64733276e-02  3.39314073e-01\n",
      "  -1.24447443e-01 -6.02808952e-01 -1.31329417e+00 -2.52023935e-01\n",
      "  -1.04210651e+00 -1.28538668e+00 -4.44637209e-01  1.38656050e-01\n",
      "   2.24620506e-01  2.20691055e-01 -2.93923140e-01  3.47230658e-02\n",
      "  -2.68138498e-01  2.52771705e-01  8.13671410e-01 -2.96582133e-01\n",
      "   9.17571068e-01 -4.60455835e-01 -4.12233740e-01 -6.67553902e-01\n",
      "   8.68172497e-02  1.60344809e-01 -1.63572574e+00 -3.02731514e-01\n",
      "  -8.21481109e-01 -6.96784794e-01  1.73272774e-01  4.73819494e-01\n",
      "  -3.36352438e-01 -5.97198546e-01 -5.73052883e-01 -3.03226531e-01\n",
      "   4.93410200e-01 -9.86553848e-01  1.13781428e+00  2.27786809e-01\n",
      "   5.98226130e-01 -3.63629431e-01 -4.52825308e-01 -2.30596531e-02\n",
      "  -7.20068395e-01  5.94032884e-01  6.28297180e-02 -1.67542100e+00\n",
      "   6.84947014e-01  3.15437824e-01  1.30231786e+00  5.92271149e-01\n",
      "   5.25537372e-01 -5.70111394e-01 -2.30185956e-01  4.87738438e-02\n",
      "  -1.46310961e+00  2.91679084e-01  5.30582249e-01  4.55556393e-01\n",
      "  -1.05712247e+00  1.30680811e+00 -5.77879958e-02  9.02808756e-02\n",
      "  -4.90737140e-01  2.87301540e-01  2.68229067e-01  6.72244370e-01\n",
      "   1.36548173e+00  5.68998465e-03 -7.43365884e-01  1.22896111e+00\n",
      "  -6.58429444e-01  3.99420738e-01 -1.41790360e-01  2.50783741e-01\n",
      "   1.06889620e-01  1.99268937e-01 -1.92880601e-01  9.59797740e-01\n",
      "  -6.98571682e-01 -7.77557731e-01 -3.92656565e-01  3.84374976e-01\n",
      "   9.25755680e-01 -7.16385424e-01 -2.27031678e-01 -1.80263236e-01\n",
      "   8.75892162e-01  6.64648831e-01 -4.05385718e-02 -3.86091843e-02\n",
      "  -3.71947318e-01 -9.97883976e-01  7.03277946e-01 -4.95072603e-01\n",
      "  -1.86440855e-01 -5.57139456e-01 -5.84804416e-01  8.57470691e-01\n",
      "   9.73085046e-01 -4.38927524e-02  2.94236634e-02 -5.07004917e-01\n",
      "   5.19699454e-01  6.78202927e-01 -7.75590301e-01 -2.01014489e-01\n",
      "  -6.46362007e-02  1.69210166e-01 -3.15657377e-01 -6.38073832e-02\n",
      "   4.14385498e-01 -7.92833328e-01  7.81127334e-01  4.93746281e-01\n",
      "  -1.67177573e-01  3.25104475e-01  4.08136785e-01 -1.98128253e-01\n",
      "  -1.57883692e+00 -3.42696816e-01  2.81852067e-01  1.56250238e+00\n",
      "   5.42061515e-02 -3.72796446e-01  3.45635563e-01  2.87225097e-02\n",
      "   3.25957209e-01  4.45555866e-01 -2.36086305e-02 -3.59087408e-01\n",
      "   2.44493052e-01  1.08073816e-01 -2.71455641e-03  8.00017536e-01\n",
      "   1.64834112e-01 -7.84870610e-02  6.20461583e-01 -8.95981789e-01\n",
      "   9.13145900e-01  4.57475752e-01  4.43795383e-01  9.52291012e-01\n",
      "   5.36398530e-01  2.86281794e-01 -1.04407489e+00  6.62327230e-01\n",
      "  -4.04090971e-01 -4.31125253e-01  8.86067092e-01 -4.57726866e-01\n",
      "  -5.38968027e-01  4.08299029e-01  8.11756492e-01  3.18704307e-01\n",
      "  -3.00523788e-01 -2.06661388e-01 -7.86023855e-01 -5.08803785e-01\n",
      "   3.13620389e-01 -8.04826140e-01 -1.63368911e-01  8.13571811e-01\n",
      "   1.00145742e-01  4.00417656e-01  7.28012979e-01  5.52753866e-01\n",
      "   3.58506352e-01 -1.27684802e-01  1.08199143e+00 -3.31548452e-01\n",
      "  -2.09918946e-01  7.80606449e-01  4.58833396e-01  3.08564186e-01\n",
      "  -1.65190959e+00 -2.91311920e-01  2.00889155e-01 -7.44069070e-02\n",
      "  -8.22851509e-02 -7.58320689e-01  9.04613793e-01  1.21438906e-01\n",
      "   6.82858348e-01  2.89186031e-01  4.92761523e-01  3.60833466e-01\n",
      "   7.89761364e-01 -6.59668803e-01  2.36380249e-01 -5.94367445e-01\n",
      "  -3.39692719e-02  1.18597403e-01  1.19543690e-02  3.18183661e-01\n",
      "   2.10744828e-01 -9.42378715e-02  2.62368381e-01  6.69895947e-01\n",
      "  -1.56853199e-01  1.33296382e+00 -2.77233962e-02 -3.85722101e-01\n",
      "  -1.11986542e+00 -1.05736136e+00 -1.63956448e-01 -3.50023478e-01\n",
      "   5.55419087e-01  8.63680989e-02  2.35156957e-02  8.91652465e-01\n",
      "  -8.80943313e-02  4.73888814e-02  2.20449254e-01  4.89862151e-02\n",
      "  -3.63904893e-01  9.15377259e-01  6.34499034e-03  2.01183602e-01\n",
      "  -1.10757411e+00  1.27832341e+00  3.28630656e-01  1.38878059e+00\n",
      "   8.49071294e-02  1.00432768e-01  3.45135361e-01 -2.10845545e-01\n",
      "  -1.00216460e+00  5.91331184e-01  2.21506506e-01 -2.87746966e-01\n",
      "   1.82966739e-01 -7.25790381e-01 -4.80665788e-02 -2.03027770e-01\n",
      "   4.38332371e-02 -8.87951195e-01 -1.55841804e+00 -4.66843367e-01\n",
      "  -8.82859886e-01  7.36437440e-01  4.56628092e-02  7.06463337e-01\n",
      "  -4.05852795e-01  6.14356220e-01  5.78302503e-01 -1.18286133e+00\n",
      "  -2.77493335e-02 -7.50104606e-01 -3.99379313e-01 -7.83226609e-01\n",
      "  -2.69309670e-01  4.57273901e-01  4.13473696e-01  6.31410718e-01\n",
      "  -2.75247961e-01 -7.64645636e-01 -1.52929902e+00  3.75116169e-01\n",
      "  -6.66492343e-01  3.35961819e-01 -9.38857853e-01 -7.68575609e-01\n",
      "  -2.29994342e-01 -6.83000088e-01 -2.81757057e-01 -1.94811821e-03\n",
      "   3.74063373e-01  2.74860859e-01  6.72254503e-01  5.03118873e-01\n",
      "  -9.38203156e-01 -9.07174945e-01 -1.03282106e+00 -6.58716321e-01\n",
      "  -6.61198735e-01 -1.01838470e+00  3.76534075e-01  1.15521634e+00\n",
      "  -7.43075728e-01 -7.23348260e-01 -9.15834129e-01  5.64644575e-01\n",
      "   4.47727025e-01  6.31319821e-01  1.31399389e-02 -2.28386492e-01\n",
      "  -2.94719607e-01  6.34674191e-01  2.57517368e-01 -4.38147455e-01\n",
      "   6.84132099e-01  5.28099179e-01 -6.23339534e-01 -9.34386626e-02\n",
      "   6.24624789e-01 -1.16408491e+00 -9.08659577e-01  4.05243099e-01\n",
      "  -1.46841908e+00  3.72077644e-01 -7.48039111e-02  3.50936145e-01\n",
      "   8.60909760e-01  4.29133594e-01 -1.42858133e-01  1.14047766e-01\n",
      "   3.74012180e-02 -3.59079748e-01 -4.20906395e-03 -2.62010753e-01\n",
      "   3.37051868e-01  1.58273470e+00 -6.01672530e-01 -2.27530763e-01\n",
      "   1.19591618e+00  6.50878429e-01 -8.92954618e-02 -4.18905675e-01\n",
      "   1.57126594e+00 -3.31305087e-01 -2.72812784e-01  1.76590532e-01\n",
      "  -7.38165751e-02  3.12449396e-01 -8.96948564e-04 -2.13333085e-01\n",
      "  -1.53492773e+00  9.10115838e-02  2.28632420e-01 -1.51050591e+00\n",
      "  -3.86804968e-01  2.57272094e-01  8.85265529e-01 -4.24517930e-01\n",
      "  -3.86550158e-01  8.38054836e-01 -2.59798110e-01  5.40384293e-01\n",
      "  -2.86054909e-01  5.55786610e-01  6.21417701e-01 -1.23508692e-01\n",
      "   1.77525636e-02 -5.28725982e-01 -1.20459870e-01  2.90037692e-01\n",
      "   2.68551350e-01  1.10356368e-01 -1.02779257e+00 -9.56030190e-01\n",
      "   4.13787931e-01  2.25953862e-01 -1.31037712e+00 -2.35217905e+00\n",
      "  -1.86778069e-01 -1.20109797e+00 -4.42366362e-01 -8.44804585e-01\n",
      "  -3.90224129e-01  6.69505954e-01 -1.70657545e-01 -3.98036558e-03\n",
      "  -2.59338528e-01  3.82893682e-01 -5.44768810e-01 -6.13513663e-02\n",
      "  -2.61051238e-01 -3.40792984e-01 -9.61913466e-02  5.89638054e-02\n",
      "   3.95657361e-01 -5.89383304e-01  2.99506247e-01 -1.74587905e-01\n",
      "   2.27259755e-01 -8.18412960e-01 -7.32798457e-01  2.97234714e-01\n",
      "   2.28747055e-01  9.10663009e-01 -6.07199296e-02 -4.96814013e-01\n",
      "  -1.18733108e-01 -7.73223400e-01 -2.96833932e-01  6.71157002e-01\n",
      "  -3.38998139e-01 -9.53684896e-02 -5.03132224e-01  2.47434378e-01\n",
      "   4.27117407e-01  1.45333424e-01  8.05351079e-01 -8.57041955e-01\n",
      "   7.25859523e-01 -9.15386230e-02  2.53698140e-01  1.85381621e-01\n",
      "  -5.47813058e-01 -5.04315197e-01  1.36256063e+00 -8.62131953e-01\n",
      "   6.29885569e-02 -1.83469206e-01 -7.75032163e-01  3.41291696e-01\n",
      "   5.35521626e-01 -1.12125719e+00 -4.04237211e-03  1.34707645e-01\n",
      "   1.16121970e-01 -3.41756701e-01  9.87665176e-01 -3.03870976e-01\n",
      "  -1.51143968e-01 -1.04746342e+00 -4.28524196e-01 -4.62444961e-01\n",
      "   7.40948081e-01  9.54195023e-01 -8.21143270e-01  8.59958649e-01\n",
      "  -2.04631686e-03  3.10486525e-01  5.44805229e-01 -9.86975670e-01\n",
      "  -8.76125693e-01 -4.50644821e-01  4.49649006e-01  4.62937295e-01\n",
      "   3.74305099e-01  1.16935861e+00 -7.40901828e-01  2.80614078e-01\n",
      "  -2.25815829e-02 -1.63027835e+00  4.21418816e-01  8.17476571e-01\n",
      "   5.69595218e-01  8.26957375e-02  9.53551471e-01  3.85207981e-01\n",
      "  -1.18330598e+00  7.06427276e-01  6.79352403e-01 -1.24054742e+00\n",
      "  -1.43386650e+00 -5.74115753e-01 -2.98397392e-01 -4.57422167e-01\n",
      "  -6.46811843e-01 -1.45287305e-01  5.62383175e-01  2.07257912e-01\n",
      "   8.17670643e-01  6.23559535e-01  2.27316618e-02  1.39370933e-01\n",
      "  -6.04807675e-01  2.03888625e-01  4.75690901e-01 -4.73644733e-01\n",
      "  -6.28172994e-01  3.59570026e-01  2.00764626e-01  3.36479664e-01\n",
      "   2.37539575e-01 -3.53950769e-01  1.01171575e-01 -2.28819698e-01\n",
      "  -2.18077824e-01 -2.69778639e-01  9.67436969e-01  2.01077133e-01\n",
      "  -1.27904445e-01  7.99281657e-01 -1.35350928e-01 -3.20220053e-01\n",
      "   2.12029099e-01 -2.10678861e-01 -7.97776520e-01  3.23254853e-01\n",
      "  -9.47452426e-01  8.75161946e-01  4.30704743e-01 -2.36586571e-01\n",
      "  -7.85881937e-01  4.22883220e-02 -1.80885568e-01  2.75950436e-03\n",
      "  -6.27907276e-01  6.07977867e-01  1.15655136e+00  8.13454986e-01\n",
      "  -6.18542075e-01 -1.47315249e-01 -5.30723631e-01  9.00456727e-01\n",
      "  -5.07012486e-01 -3.95729512e-01 -3.64806540e-02 -4.39003795e-01\n",
      "  -5.66664152e-02 -7.64992356e-01 -1.15275955e+00 -5.64237714e-01\n",
      "   2.25648016e-01 -8.70941162e-01 -1.07499719e+00 -3.86827072e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Austral",
   "language": "python",
   "name": "austral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
